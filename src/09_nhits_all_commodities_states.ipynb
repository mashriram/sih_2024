{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = \"../../sih_2024_data_source/statewise_results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dict()\n",
    "\n",
    "\n",
    "def get_scores(y_test, y_pred):\n",
    "    r2_ = r2_score(y_test, y_pred)\n",
    "    rmse_ = root_mean_squared_error(y_test, y_pred)\n",
    "    mae_ = mean_absolute_error(y_test, y_pred)\n",
    "    return {\"r2\": r2_, \"mae\": mae_, \"rmse\": rmse_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS, NHITS, DeepAR, TFT, LSTM, RNN, GRU\n",
    "from neuralforecast.losses.pytorch import DistributionLoss, MAE, MSE, MAPE, SMAPE\n",
    "import torch\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import (\n",
    "    NHiTSModel,)\n",
    "\n",
    "def create_darts_models(input_chunk_length=120, output_chunk_length=30, n_epochs=100):\n",
    "    \"\"\"\n",
    "    Create a collection of Darts models with correct parameters\n",
    "    \"\"\"\n",
    "    # Common parameters for neural networks\n",
    "    nn_params = {\n",
    "        \"input_chunk_length\": input_chunk_length,\n",
    "        \"output_chunk_length\": output_chunk_length,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"batch_size\": 32,\n",
    "        \"force_reset\": True,\n",
    "    }\n",
    "\n",
    "    models = {\n",
    "       \n",
    "        \"nhits\": NHiTSModel(\n",
    "            **nn_params,\n",
    "            num_stacks=3,\n",
    "            num_blocks=1,\n",
    "            num_layers=2,\n",
    "            layer_widths=512,\n",
    "            pooling_kernel_sizes=None,\n",
    "            n_freq_downsample=None,\n",
    "            dropout=0.1,\n",
    "            activation=\"ReLU\",\n",
    "            MaxPool1d=True,\n",
    "        ),\n",
    "               \n",
    "    }\n",
    "\n",
    "    return models\n",
    "\n",
    "def train_and_forecast(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Train models and generate forecasts using either Nixtla or Darts\n",
    "    \"\"\"\n",
    "\n",
    "    # Darts workflow\n",
    "    # Convert pandas DataFrame to Darts TimeSeries\n",
    "    series = TimeSeries.from_dataframe(df_train, \"ds\", \"y\",fill_missing_dates=True, freq=None)\n",
    "\n",
    "    # Create and train models\n",
    "    models = create_darts_models()\n",
    "    forecasts = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name} model...\")\n",
    "        model.fit(series)\n",
    "        forecast = model.predict(len(df_test))\n",
    "        \n",
    "        forecasts[name] = {\"data\":forecast,\"model\":model}\n",
    "\n",
    "    return forecasts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_interpolated_ranges(dataframe,date_col,value_col):\n",
    "    dataframe[date_col] = pd.to_datetime(dataframe[date_col])\n",
    "    date_range = pd.date_range(start=dataframe[date_col].min(), end=dataframe[date_col].max())\n",
    "    full_df = pd.DataFrame({date_col: date_range})\n",
    "    merged_df = pd.merge(full_df, dataframe, on=date_col, how='left')\n",
    "    merged_df[value_col] = merged_df[value_col].interpolate()\n",
    "    merged_df[value_col] = merged_df[value_col].fillna(method='bfill').fillna(method='ffill')\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_283559/327143356.py:7: FutureWarning:\n",
      "\n",
      "Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "\n",
      "/tmp/ipykernel_283559/327143356.py:7: FutureWarning:\n",
      "\n",
      "Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 968 K  | train\n",
      "-------------------------------------------------------------\n",
      "907 K     Trainable params\n",
      "61.6 K    Non-trainable params\n",
      "968 K     Total params\n",
      "3.876     Total estimated model params size (MB)\n",
      "42        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masur dal\n",
      "(472, 2) (2290, 2)\n",
      "['2014-09-01' '2014-09-02' '2014-09-03' ... '2023-05-15' '2023-05-16'\n",
      " '2023-05-17']\n",
      "           ds            y\n",
      "0  2014-09-01  6100.000000\n",
      "1  2014-09-02  5425.000000\n",
      "2  2014-09-03  5312.500000\n",
      "3  2014-09-04  5706.250000\n",
      "4  2014-09-05  6100.000000\n",
      "5  2014-09-06  5425.000000\n",
      "6  2014-09-07  4450.000000\n",
      "7  2014-09-08  6175.000000\n",
      "8  2014-09-09  5480.000000\n",
      "9  2014-09-10  5287.500000\n",
      "10 2014-09-11  6175.000000\n",
      "11 2014-09-12  5512.500000\n",
      "12 2014-09-13  4850.000000\n",
      "13 2014-09-14  4450.000000\n",
      "14 2014-09-15  6190.000000\n",
      "15 2014-09-16  6200.000000\n",
      "16 2014-09-17  5340.000000\n",
      "17 2014-09-18  6200.000000\n",
      "18 2014-09-19  5656.666667\n",
      "19 2014-09-20  5113.333333\n",
      "Training nhits model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ed2317b54146ccac0d1ba974ae60f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea4767bd5564766bf6a008a5ed113aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nhits': {'data': <TimeSeries (DataArray) (ds: 474, component: 1, sample: 1)> Size: 4kB\n",
      "array([[[8280.9573568 ]],\n",
      "\n",
      "       [[8042.49521448]],\n",
      "\n",
      "       [[8211.87203618]],\n",
      "\n",
      "       [[8367.67024876]],\n",
      "\n",
      "       [[8152.1777231 ]],\n",
      "\n",
      "       [[8244.03373449]],\n",
      "\n",
      "       [[8179.7940839 ]],\n",
      "\n",
      "       [[8303.70533694]],\n",
      "\n",
      "       [[8091.52649214]],\n",
      "\n",
      "       [[8222.23531568]],\n",
      "\n",
      "...\n",
      "\n",
      "       [[6161.91480013]],\n",
      "\n",
      "       [[6133.79681261]],\n",
      "\n",
      "       [[6189.91066976]],\n",
      "\n",
      "       [[6187.05537323]],\n",
      "\n",
      "       [[6163.62787101]],\n",
      "\n",
      "       [[6132.5225462 ]],\n",
      "\n",
      "       [[6160.59801986]],\n",
      "\n",
      "       [[6105.17460864]],\n",
      "\n",
      "       [[6050.08444573]],\n",
      "\n",
      "       [[6089.30653557]]])\n",
      "Coordinates:\n",
      "  * ds         (ds) datetime64[ns] 4kB 2023-05-18 2023-05-19 ... 2024-09-02\n",
      "  * component  (component) object 8B 'y'\n",
      "Dimensions without coordinates: sample\n",
      "Attributes:\n",
      "    static_covariates:  None\n",
      "    hierarchy:          None, 'model': NHiTSModel(output_chunk_shift=0, num_stacks=3, num_blocks=1, num_layers=2, layer_widths=512, pooling_kernel_sizes=None, n_freq_downsample=None, dropout=0.1, activation=ReLU, MaxPool1d=True, input_chunk_length=120, output_chunk_length=30, n_epochs=100, batch_size=32, force_reset=True)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_283559/327143356.py:7: FutureWarning:\n",
      "\n",
      "Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "\n",
      "/tmp/ipykernel_283559/327143356.py:7: FutureWarning:\n",
      "\n",
      "Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 968 K  | train\n",
      "-------------------------------------------------------------\n",
      "907 K     Trainable params\n",
      "61.6 K    Non-trainable params\n",
      "968 K     Total params\n",
      "3.876     Total estimated model params size (MB)\n",
      "42        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(644, 2) (2007, 2)\n",
      "['2014-09-01' '2014-09-06' '2014-09-09' ... '2022-01-31' '2022-02-01'\n",
      " '2022-02-02']\n",
      "           ds       y\n",
      "0  2014-09-01  6350.0\n",
      "1  2014-09-02  6290.0\n",
      "2  2014-09-03  6230.0\n",
      "3  2014-09-04  6170.0\n",
      "4  2014-09-05  6110.0\n",
      "5  2014-09-06  6050.0\n",
      "6  2014-09-07  6050.0\n",
      "7  2014-09-08  6050.0\n",
      "8  2014-09-09  6050.0\n",
      "9  2014-09-10  6100.0\n",
      "10 2014-09-11  6100.0\n",
      "11 2014-09-12  6100.0\n",
      "12 2014-09-13  6100.0\n",
      "13 2014-09-14  6162.5\n",
      "14 2014-09-15  6225.0\n",
      "15 2014-09-16  6350.0\n",
      "16 2014-09-17  6225.0\n",
      "17 2014-09-18  5825.0\n",
      "18 2014-09-19  6300.0\n",
      "19 2014-09-20  6225.0\n",
      "Training nhits model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338129511cce4022aa682a38e1d2c7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ast import mod\n",
    "import os\n",
    "from re import I\n",
    "\n",
    "ers = {}\n",
    "for commodity in os.listdir(DATA_SOURCE):\n",
    "    print(commodity)\n",
    "    ers[commodity] = {}\n",
    "    path = DATA_SOURCE + commodity\n",
    "    for state_csv in os.listdir(path):\n",
    "        sub_path = path + '/'+ state_csv\n",
    "        state = state_csv.partition(\"_\")[0]\n",
    "        df = pd.read_csv(sub_path)\n",
    "        # df['datetime'] = pd.to_datetime(df['date'])\n",
    "        df.drop(columns=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "        df.sort_values(by=\"datetime\", ascending=True, inplace=True)\n",
    "        # print(df.head())\n",
    "        TRAIN_LEN = int(0.8 * len(df))\n",
    "        df_train, df_test = (df[:TRAIN_LEN],df[TRAIN_LEN:])\n",
    "        if(df_train.shape[0]<151  or df_test.shape[0]<15):\n",
    "            continue\n",
    "        df_train.set_index('datetime', inplace=True)\n",
    "        df_train.sort_index(inplace=True)\n",
    "        df_test.set_index('datetime', inplace=True)\n",
    "        df_test.sort_index(inplace=True)\n",
    "        df_train_dt = df_train.groupby(\"datetime\").agg( {\"modal_rs_quintal\": \"mean\"})\n",
    "        df_test_dt = df_test.groupby(\"datetime\").agg( {\"modal_rs_quintal\": \"mean\"})\n",
    "        df_train_dt.reset_index(inplace=True)\n",
    "        df_train_dt.rename(\n",
    "            columns={\"datetime\": \"ds\", \"modal_rs_quintal\": \"y\"}, inplace=True\n",
    "        )\n",
    "        df_test_dt.reset_index(inplace=True)\n",
    "        df_test_dt.rename(\n",
    "            columns={\"datetime\": \"ds\", \"modal_rs_quintal\": \"y\"}, inplace=True\n",
    "        )\n",
    "        ######\n",
    "        print(df_test_dt.shape,df_train_dt.shape)\n",
    "        print(df_train_dt['ds'].unique())\n",
    "        df_train_dt = create_interpolated_ranges(df_train_dt,\"ds\",\"y\")\n",
    "        df_test_dt = create_interpolated_ranges(df_test_dt,\"ds\",\"y\")\n",
    "        print(df_train_dt.head(20))\n",
    "        if(df_train_dt.shape[0]<151  or df_test_dt.shape[0]<15):\n",
    "            continue\n",
    "        nhits_forecast = train_and_forecast(df_train=df_train_dt,df_test=df_test_dt)\n",
    "        print(nhits_forecast)\n",
    "        for name,data_model in nhits_forecast.items():\n",
    "            os.makedirs(f'./model_results/{commodity}/{state}/',exist_ok=True)\n",
    "            nhits_forecast[name][\"model\"].save(f'./model_results/{commodity}/{state}/nhits.pkt')\n",
    "            nhits_forecast[name] = pd.DataFrame(nhits_forecast[name]['data'].values())[0]\n",
    "        result = pd.DataFrame(nhits_forecast)\n",
    "        result_y = df_test_dt['y']\n",
    "        results = pd.concat([result,result_y],axis= 1)\n",
    "        for column in results.columns:\n",
    "            scores[column] = get_scores(results[\"y\"], results[column])\n",
    "        \n",
    "\n",
    "        results.to_csv(f'./model_results/{commodity}/results.csv')\n",
    "        error_results = pd.DataFrame(scores)\n",
    "        error_results.to_csv(f\"./model_results/{commodity}/errors.csv\")\n",
    "        ers[state] = {'results':results,'error_results':error_results}\n",
    "        px.line(\n",
    "        results,\n",
    "        x=results.index,\n",
    "        y=[\n",
    "            \"y\",\n",
    "            \"nhits\",\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
